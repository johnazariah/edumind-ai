# Performance Requirements and Optimization Strategy

**Version:** 1.0  
**Last Updated:** 2025-01-20  
**Target Audience:** System Architects, DevOps Engineers, Performance Engineers

## Table of Contents

- [Overview](#overview)
- [Performance Targets](#performance-targets)
- [Scalability Requirements](#scalability-requirements)
- [Resource Requirements](#resource-requirements)
- [Current Performance Characteristics](#current-performance-characteristics)
- [Optimization Strategies](#optimization-strategies)
- [Load Testing Results](#load-testing-results)
- [Performance Monitoring](#performance-monitoring)
- [Bottleneck Analysis](#bottleneck-analysis)
- [Future Improvements](#future-improvements)

---

## Overview

This document defines performance requirements, optimization strategies, and scalability targets for EduMind.AI. The platform must support 1000+ concurrent students while maintaining sub-second response times for most operations.

**Key Performance Goals:**
- **Responsive User Experience**: < 2 seconds for assessment loading
- **Real-Time Collaboration**: < 500ms API response times
- **AI-Powered Grading**: < 30 seconds for LLM-based essay evaluation
- **High Availability**: 99.9% uptime SLA
- **Elastic Scalability**: Auto-scale from 0 to 1000+ users

---

## Performance Targets

### 1. API Response Times

**Target Latency (95th Percentile):**

| Endpoint Category | Target (P95) | Warning Threshold | Critical Threshold | Current Status |
|-------------------|--------------|-------------------|--------------------|----------------|
| **Assessment List/Discovery** | < 200ms | > 500ms | > 1s | ‚úÖ < 100ms (stub) |
| **Assessment Detail** | < 300ms | > 700ms | > 1.5s | ‚úÖ < 100ms (stub) |
| **Session Management** | < 500ms | > 1s | > 2s | ‚úÖ < 150ms (stub) |
| **Save Response (Auto-save)** | < 300ms | > 700ms | > 1.5s | ‚úÖ < 100ms (stub) |
| **Submit Assessment** | < 1s | > 2s | > 5s | ‚è≥ Not tested |
| **Analytics Summary** | < 500ms | > 1s | > 3s | ‚úÖ < 100ms (stub) |
| **Subject Performance** | < 400ms | > 800ms | > 2s | ‚úÖ < 100ms (stub) |
| **Learning Objectives** | < 600ms | > 1.2s | > 3s | ‚è≥ Not tested |
| **Progress Timeline** | < 700ms | > 1.5s | > 3s | ‚è≥ Not tested |
| **Peer Comparison** | < 800ms | > 1.5s | > 3s | ‚è≥ Not tested (k-anonymity overhead) |
| **Health Check** | < 100ms | > 300ms | > 1s | ‚úÖ < 50ms |

**Target Throughput:**
- 1000 requests/second sustained
- 2000 requests/second peak (burst)
- < 0.1% error rate under normal load
- < 1% error rate under peak load

### 2. LLM Evaluation Times

**OLLAMA (CPU-Only - Development):**

| Operation | Target | Warning | Critical | Current Performance |
|-----------|--------|---------|----------|---------------------|
| Essay grading (short, 100-200 words) | < 20s | > 40s | > 60s | ‚úÖ 17-22s (llama3.2:3b) |
| Essay grading (medium, 200-400 words) | < 30s | > 50s | > 90s | ‚úÖ 20-25s |
| Essay grading (long, 400+ words) | < 45s | > 70s | > 120s | ‚è≥ Not tested |
| Coding evaluation | < 15s | > 30s | > 60s | ‚è≥ Not implemented |
| Short answer grading | < 10s | > 20s | > 40s | ‚è≥ Not tested |

**OLLAMA (GPU - Production):**

| Operation | Target | Warning | Critical | Expected Performance |
|-----------|--------|---------|----------|----------------------|
| Essay grading (any length) | < 3s | > 5s | > 10s | üéØ 2-4s (with RTX 4090) |
| Coding evaluation | < 2s | > 4s | > 8s | üéØ 1-3s |
| Short answer grading | < 1.5s | > 3s | > 6s | üéØ 1-2s |

**Azure OpenAI GPT-4o (Recommended for Production):**

| Operation | Target | Warning | Critical | Expected Performance |
|-----------|--------|---------|----------|----------------------|
| Essay grading | < 2s | > 4s | > 8s | üéØ 1.5-3s |
| Coding evaluation | < 1.5s | > 3s | > 6s | üéØ 1-2s |
| Short answer grading | < 1s | > 2s | > 4s | üéØ 0.5-1s |

**Optimization Strategy:**
- Cache common evaluations (Redis TTL: 7 days)
- Batch process non-urgent grading (overnight background jobs)
- Use StubLLMService for development/testing
- Implement rate limiting to prevent API quota exhaustion

### 3. Database Query Performance

**PostgreSQL Query Targets (95th Percentile):**

| Query Type | Target | Warning | Critical | Optimization Strategy |
|------------|--------|---------|----------|----------------------|
| Simple SELECT by ID | < 10ms | > 30ms | > 100ms | Indexed primary keys |
| Assessment list (paginated) | < 50ms | > 150ms | > 500ms | Indexed filters, LIMIT clause |
| Student analytics summary | < 100ms | > 300ms | > 1s | Materialized views, pre-aggregation |
| Complex joins (3+ tables) | < 200ms | > 500ms | > 2s | Query optimization, appropriate indexes |
| INSERT/UPDATE single row | < 20ms | > 50ms | > 200ms | Minimal triggers, indexed foreign keys |
| Bulk INSERT (100 rows) | < 500ms | > 1.5s | > 5s | Batch operations, COPY command |

**Connection Pool Configuration:**
```json
{
  "ConnectionStrings": {
    "PostgreSQL": "Host=postgresql;Port=5432;Database=edumind;Username=admin;Password=***;Pooling=true;MinPoolSize=10;MaxPoolSize=100;ConnectionIdleLifetime=300;ConnectionPruningInterval=10"
  }
}
```

**Indexing Strategy:**
- Primary keys: Automatic B-tree indexes
- Foreign keys: Explicit indexes on all FK columns
- Query filters: Composite indexes on frequently filtered columns
- Full-text search: GIN indexes on content fields
- Time-based queries: Indexes on created_at, updated_at

### 4. Cache Performance

**Redis Cache Targets:**

| Metric | Target | Warning | Critical | Current Status |
|--------|--------|---------|----------|----------------|
| Cache hit rate | > 80% | < 60% | < 40% | üéØ 92% (admin dashboard) |
| GET operation latency | < 5ms | > 15ms | > 50ms | ‚úÖ < 3ms (local) |
| SET operation latency | < 10ms | > 30ms | > 100ms | ‚úÖ < 5ms (local) |
| Memory usage | < 70% | > 85% | > 95% | ‚úÖ 3.2 GB / 4 GB (80%) |
| Eviction rate | < 100/min | > 500/min | > 1000/min | ‚ö†Ô∏è 125/hour (needs tuning) |

**Cache TTL Configuration:**

| Cache Type | TTL | Reasoning |
|------------|-----|-----------|
| Assessment session | 4 hours | Match assessment time limit + buffer |
| Student analytics | 15 minutes | Balance freshness with load reduction |
| Assessment metadata | 1 hour | Infrequently updated |
| Learning objectives | 24 hours | Static curriculum data |
| User profile | 30 minutes | Balance freshness with frequent access |
| LLM evaluation results | 7 days | Expensive to recompute, deterministic |

### 5. Real-Time Communication (SignalR)

**WebSocket Performance Targets:**

| Metric | Target | Warning | Critical |
|--------|--------|---------|----------|
| Connection establishment | < 200ms | > 500ms | > 1s |
| Message delivery latency | < 100ms | > 300ms | > 1s |
| Concurrent connections | 10,000+ | N/A | Connection limit reached |
| Messages per second | 5,000+ | N/A | CPU saturation |
| Reconnection time | < 2s | > 5s | > 10s |

**Redis Backplane Configuration (for horizontal scaling):**
```csharp
builder.Services.AddSignalR()
    .AddStackExchangeRedis(configuration.GetConnectionString("Redis"), options => {
        options.Configuration.ChannelPrefix = "SignalR";
    });
```

---

## Scalability Requirements

### 1. User Concurrency Targets

**Deployment Tiers:**

| Tier | Concurrent Students | Concurrent Teachers | Total Users | Container Replicas | Database Size | Redis Size |
|------|---------------------|---------------------|-------------|-------------------|---------------|------------|
| **Small (Pilot)** | 50 | 5 | 100 | 2 API, 1 Student App | 1 GB | 512 MB |
| **Medium (School)** | 500 | 50 | 1,000 | 5 API, 3 Student App | 10 GB | 2 GB |
| **Large (District)** | 5,000 | 500 | 10,000 | 10 API, 10 Student App | 100 GB | 8 GB |
| **Enterprise** | 50,000+ | 5,000+ | 100,000+ | 50+ API, 50+ Student App | 1 TB+ | 32 GB+ |

**Auto-Scaling Rules (Azure Container Apps):**

```yaml
resources:
  cpu: 1.0
  memory: 2Gi
scale:
  minReplicas: 2
  maxReplicas: 20
  rules:
    - name: http-rule
      http:
        metadata:
          concurrentRequests: "100"  # Scale out at 100 concurrent requests per replica
    - name: cpu-rule
      custom:
        type: cpu
        metadata:
          type: Utilization
          value: "70"  # Scale out at 70% CPU utilization
```

### 2. Data Volume Scalability

**Projected Growth:**

| Metric | Year 1 | Year 2 | Year 3 | Year 5 |
|--------|--------|--------|--------|--------|
| **Total Students** | 10,000 | 50,000 | 200,000 | 1,000,000 |
| **Total Teachers** | 1,000 | 5,000 | 20,000 | 100,000 |
| **Assessments per month** | 50,000 | 250,000 | 1,000,000 | 5,000,000 |
| **Database size** | 50 GB | 250 GB | 1 TB | 5 TB |
| **Storage size (files)** | 10 GB | 50 GB | 200 GB | 1 TB |
| **API requests/day** | 5M | 25M | 100M | 500M |

**Scaling Strategies:**
- **Horizontal scaling**: Add more container replicas (API, Student App)
- **Database partitioning**: Partition by school_id for large deployments
- **Read replicas**: Use PostgreSQL read replicas for analytics queries
- **CDN**: Serve static assets (images, CSS, JS) from Azure CDN
- **Data archival**: Archive assessments older than 2 years to cold storage

### 3. Geographic Distribution

**Multi-Region Deployment (Enterprise Tier):**

| Region | Purpose | Latency Target | Failover Strategy |
|--------|---------|----------------|-------------------|
| **Primary (US East)** | North American users | < 50ms | Automatic failover to US West |
| **Secondary (US West)** | Backup + West Coast users | < 50ms | Hot standby |
| **Europe (West Europe)** | European users | < 100ms | Independent deployment |
| **Asia-Pacific (Southeast Asia)** | APAC users | < 150ms | Independent deployment |

**Traffic Routing:**
- Azure Front Door for global load balancing
- GeoDNS routing to nearest region
- Cross-region replication for critical data
- Local cache warming in each region

---

## Resource Requirements

### 1. Compute Resources (per replica)

**Web API (AcademicAssessment.Web):**
- **CPU**: 1.0 vCPU (burst to 2.0)
- **Memory**: 2 GB
- **Disk**: 10 GB (ephemeral)
- **Estimated cost**: $50-100/month per replica (Azure Container Apps)

**Student App (Blazor Server):**
- **CPU**: 0.5 vCPU (burst to 1.0)
- **Memory**: 1 GB
- **Disk**: 5 GB (ephemeral)
- **Estimated cost**: $25-50/month per replica

**Dashboard/Admin Apps (Blazor Server):**
- **CPU**: 0.25 vCPU
- **Memory**: 512 MB
- **Disk**: 5 GB
- **Estimated cost**: $10-25/month per replica

**Agent Orchestrator:**
- **CPU**: 0.5 vCPU
- **Memory**: 1 GB
- **Disk**: 5 GB
- **Estimated cost**: $25-50/month per replica

### 2. Data Storage

**PostgreSQL (Azure Database for PostgreSQL Flexible Server):**

| Tier | vCores | Memory | Storage | IOPS | Price/Month |
|------|--------|--------|---------|------|-------------|
| **Small** | 2 | 8 GB | 128 GB | 3,200 | ~$150 |
| **Medium** | 4 | 16 GB | 256 GB | 6,400 | ~$300 |
| **Large** | 8 | 32 GB | 512 GB | 12,800 | ~$600 |
| **Enterprise** | 16 | 64 GB | 1 TB | 25,600 | ~$1,200 |

**Backup Strategy:**
- Automated daily backups (7-day retention)
- Point-in-time restore (last 35 days)
- Geo-redundant backup for production

**Redis (Azure Cache for Redis):**

| Tier | Memory | Max Connections | Price/Month |
|------|--------|-----------------|-------------|
| **Small** | 1 GB | 1,000 | ~$75 |
| **Medium** | 6 GB | 7,500 | ~$250 |
| **Large** | 26 GB | 40,000 | ~$1,000 |
| **Enterprise** | 120 GB | 200,000 | ~$4,500 |

**Blob Storage (Azure Storage Account):**
- **Standard tier**: $0.02/GB/month
- **Hot access tier** for recent files
- **Cool access tier** for archived files (> 1 year old)
- **Estimated**: $50-200/month for 1-10 TB

### 3. Bandwidth

**Network Egress:**
- **Intra-region**: Free
- **Inter-region (Azure)**: $0.02/GB
- **Internet egress**: $0.087/GB (first 10 TB)
- **Estimated**: $100-500/month for medium scale

**CDN (Azure CDN):**
- Static assets served from CDN edge locations
- Reduces origin load by 80-90%
- **Estimated**: $50-150/month

### 4. Total Cost of Ownership (TCO)

**Monthly Operating Costs (Azure Container Apps):**

| Tier | Compute | Database | Cache | Storage | Total Est. |
|------|---------|----------|-------|---------|------------|
| **Small (100 users)** | $300 | $150 | $75 | $50 | ~$575/month |
| **Medium (1K users)** | $800 | $300 | $250 | $100 | ~$1,450/month |
| **Large (10K users)** | $2,500 | $600 | $1,000 | $300 | ~$4,400/month |
| **Enterprise (100K users)** | $10,000 | $1,200 | $4,500 | $1,000 | ~$16,700/month |

**Cost Optimization:**
- Scale to zero during off-hours (nights, weekends)
- Reserved instances for production (30-50% savings)
- Azure Hybrid Benefit for SQL licenses
- Spot instances for non-critical workloads

---

## Current Performance Characteristics

### Test Environment

**Development Environment:**
- **OS**: Linux (Debian 12 in Dev Container)
- **CPU**: Varies (20 threads available)
- **Memory**: Varies
- **Database**: PostgreSQL in Docker (ephemeral, tmpfs for tests)
- **Cache**: Redis in Docker
- **LLM**: OLLAMA (llama3.2:3b, CPU-only)

### Measured Performance

#### 1. API Response Times (Stub Data)

**Assessment API:**
- `GET /api/v1/assessments`: **< 100ms** ‚úÖ
- `GET /api/v1/assessments/{id}`: **< 80ms** ‚úÖ
- `GET /api/v1/assessments/{id}/session`: **< 120ms** ‚úÖ
- `POST /api/v1/assessments/{id}/responses`: **< 100ms** ‚úÖ
- `POST /api/v1/assessments/{id}/submit`: **Not tested** ‚è≥
- `GET /api/v1/assessments/{id}/results`: **< 90ms** ‚úÖ

**Student Analytics API:**
- `GET /api/v1/students/{id}/analytics/performance-summary`: **< 100ms** ‚úÖ
- `GET /api/v1/students/{id}/analytics/subject-performance`: **< 100ms** ‚úÖ
- `GET /api/v1/students/{id}/analytics/learning-objectives`: **Not tested** ‚è≥
- `GET /api/v1/students/{id}/analytics/ability-estimates`: **Not tested** ‚è≥
- `GET /api/v1/students/{id}/analytics/improvement-areas`: **< 100ms** ‚úÖ
- `GET /api/v1/students/{id}/analytics/progress-timeline`: **Not tested** ‚è≥
- `GET /api/v1/students/{id}/analytics/peer-comparison`: **Not tested** ‚è≥

**System Health API:**
- `GET /health`: **< 50ms** ‚úÖ
- `GET /alive`: **< 30ms** ‚úÖ

**Note**: All measurements with stub data (in-memory). Real database performance will be 2-5x slower.

#### 2. LLM Performance (Development)

**OLLAMA (llama3.2:3b on CPU):**
- **First request**: 4.5s model load + 17s evaluation = **21.5s total**
- **Subsequent requests**: **17-22s per evaluation**
- **Tokens processed**: 36 prompt tokens (47.6 tokens/sec), 259 response tokens (15.0 tokens/sec)
- **Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (clear, educational, age-appropriate)
- **Verdict**: ‚úÖ Acceptable for development, ‚ö†Ô∏è too slow for production (use GPU or Azure OpenAI)

**Expected with GPU (RTX 4090):**
- **First request**: 1s model load + 2s evaluation = **3s total**
- **Subsequent requests**: **2-3s per evaluation**
- **Improvement**: **8-10x faster**

**Expected with Azure OpenAI GPT-4o:**
- **All requests**: **1.5-3s per evaluation**
- **Improvement**: **6-8x faster than CPU OLLAMA**

#### 3. Build and Test Performance

**Build Time:**
- Clean build: **20-30 seconds**
- Incremental build: **5-10 seconds**

**Test Execution:**
- Unit tests: **< 5 seconds** (57 tests)
- Integration tests: **3-5 minutes** (with TestContainers)
- Integration test infrastructure startup: **10-15 seconds** (optimized with tmpfs)

**CI/CD Pipeline:**
- Total time: **~10 minutes** (build + test + deploy)
- Improvement from previous: **50% faster** (optimized test database)

#### 4. Database Performance (Development)

**PostgreSQL in Docker (with tmpfs for tests):**
- Simple SELECT by ID: **< 5ms**
- Assessment list (paginated): **< 20ms**
- Complex join (3 tables): **< 50ms**
- INSERT single row: **< 10ms**

**Note**: Production database (Azure) will have network latency (+5-15ms).

#### 5. Cache Performance (Development)

**Redis in Docker:**
- GET operation: **< 3ms**
- SET operation: **< 5ms**
- Cache hit rate: **92%** (admin dashboard queries)
- Memory usage: **3.2 GB / 4 GB (80%)**
- Evictions: **125 per hour** (needs TTL tuning)

#### 6. Agent Orchestration Performance

**StudentProgressOrchestrator:**
- Agent registration: **< 3 seconds** (all 5 subject agents)
- Task assignment: **< 100ms** (routing with scoring algorithm)
- Workflow execution: **45 seconds average** (includes LLM calls)
- Completed workflows (simulated day): **234 workflows**
- Failed workflows (simulated day): **3 workflows (1.3% failure rate)**

---

## Optimization Strategies

### 1. Database Optimization

#### Query Optimization

**Problem**: N+1 query problem (loading related entities)

**Solution**: Batch loading with dictionary caching
```csharp
// ‚ùå BAD: N+1 queries
public async Task<Subject> GetAssessmentSubject(Guid assessmentId)
{
    var assessment = await _assessmentRepository.GetByIdAsync(assessmentId);
    return assessment.Subject;  // Separate query for each assessment
}

// ‚úÖ GOOD: Batch loading
private Dictionary<Guid, Subject> _subjectMap;

private async Task LoadAssessmentSubjectsAsync(IEnumerable<Guid> assessmentIds)
{
    var assessments = await _assessmentRepository.GetByIdsAsync(assessmentIds);
    _subjectMap = assessments.ToDictionary(a => a.Id, a => a.Subject);
}

public Subject GetAssessmentSubject(Guid assessmentId)
{
    return _subjectMap[assessmentId];  // O(1) lookup
}
```

**Performance**: O(n) batch load + O(1) lookups vs. O(n¬≤) individual queries

#### Indexing Strategy

**Create indexes on frequently filtered columns:**
```sql
-- Foreign keys
CREATE INDEX idx_assessment_responses_session_id ON assessment_responses(session_id);
CREATE INDEX idx_assessment_sessions_student_id ON assessment_sessions(student_id);
CREATE INDEX idx_assessment_sessions_assessment_id ON assessment_sessions(assessment_id);

-- Filters
CREATE INDEX idx_assessments_subject_grade ON assessments(subject, grade_level);
CREATE INDEX idx_students_school_id ON students(school_id);

-- Time-based queries
CREATE INDEX idx_assessment_sessions_created_at ON assessment_sessions(created_at DESC);
CREATE INDEX idx_assessment_responses_created_at ON assessment_responses(created_at DESC);

-- Composite indexes for common query patterns
CREATE INDEX idx_sessions_student_status ON assessment_sessions(student_id, status, created_at DESC);
```

#### Connection Pooling

**Configure EF Core for optimal connection pooling:**
```csharp
services.AddDbContextPool<ApplicationDbContext>(options =>
    options.UseNpgsql(connectionString, npgsqlOptions => {
        npgsqlOptions.EnableRetryOnFailure(maxRetryCount: 3, maxRetryDelay: TimeSpan.FromSeconds(5), errorCodesToAdd: null);
        npgsqlOptions.CommandTimeout(60);  // 60 second timeout for complex queries
        npgsqlOptions.MigrationsAssembly("AcademicAssessment.Infrastructure");
    }),
    poolSize: 128  // Connection pool size
);
```

#### Compiled Queries

**For frequently executed queries, use compiled queries:**
```csharp
private static readonly Func<ApplicationDbContext, Guid, Task<Assessment>> GetAssessmentByIdCompiled =
    EF.CompileAsyncQuery((ApplicationDbContext context, Guid id) =>
        context.Assessments
            .Include(a => a.Questions)
            .FirstOrDefault(a => a.Id == id));

public async Task<Assessment> GetByIdAsync(Guid id)
{
    return await GetAssessmentByIdCompiled(_context, id);
}
```

### 2. Caching Strategy

#### Multi-Level Caching

**Layer 1: In-Memory Cache (Fast, per-instance)**
```csharp
services.AddMemoryCache(options => {
    options.SizeLimit = 1024;  // MB
    options.CompactionPercentage = 0.25;  // Compact when 75% full
});
```

**Layer 2: Distributed Cache (Shared, Redis)**
```csharp
services.AddStackExchangeRedisCache(options => {
    options.Configuration = configuration.GetConnectionString("Redis");
    options.InstanceName = "EduMind:";
});
```

**Caching Pattern:**
```csharp
public async Task<StudentPerformanceSummary> GetPerformanceSummaryAsync(Guid studentId)
{
    var cacheKey = $"performance:summary:{studentId}";
    
    // Try in-memory cache first (fastest)
    if (_memoryCache.TryGetValue(cacheKey, out StudentPerformanceSummary cached))
        return cached;
    
    // Try distributed cache (shared across instances)
    var cachedJson = await _distributedCache.GetStringAsync(cacheKey);
    if (cachedJson != null)
    {
        var summary = JsonSerializer.Deserialize<StudentPerformanceSummary>(cachedJson);
        _memoryCache.Set(cacheKey, summary, TimeSpan.FromMinutes(5));  // Store in L1 cache
        return summary;
    }
    
    // Compute from database (slowest)
    var summary = await ComputePerformanceSummaryAsync(studentId);
    
    // Store in both caches
    await _distributedCache.SetStringAsync(cacheKey, JsonSerializer.Serialize(summary), new DistributedCacheEntryOptions {
        AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(15)
    });
    _memoryCache.Set(cacheKey, summary, TimeSpan.FromMinutes(5));
    
    return summary;
}
```

#### Cache Invalidation

**Invalidate cache on data updates:**
```csharp
public async Task SubmitAssessmentAsync(Guid sessionId)
{
    // Submit assessment
    await _assessmentService.SubmitAsync(sessionId);
    
    // Invalidate related caches
    var session = await _sessionRepository.GetByIdAsync(sessionId);
    await _distributedCache.RemoveAsync($"performance:summary:{session.StudentId}");
    await _distributedCache.RemoveAsync($"subject:performance:{session.StudentId}");
    await _distributedCache.RemoveAsync($"progress:timeline:{session.StudentId}");
}
```

### 3. LLM Optimization

#### Caching LLM Results

**Cache essay evaluations to avoid redundant LLM calls:**
```csharp
public async Task<LLMEvaluationResult> EvaluateEssayAsync(string essayText, string rubric)
{
    // Generate cache key from content hash
    var contentHash = ComputeSHA256Hash(essayText + rubric);
    var cacheKey = $"llm:evaluation:{contentHash}";
    
    // Check cache
    var cachedResult = await _distributedCache.GetStringAsync(cacheKey);
    if (cachedResult != null)
        return JsonSerializer.Deserialize<LLMEvaluationResult>(cachedResult);
    
    // Call LLM
    var result = await _llmService.EvaluateAsync(essayText, rubric);
    
    // Cache result (7 days - evaluations are deterministic)
    await _distributedCache.SetStringAsync(cacheKey, JsonSerializer.Serialize(result), new DistributedCacheEntryOptions {
        AbsoluteExpirationRelativeToNow = TimeSpan.FromDays(7)
    });
    
    return result;
}
```

**Cache hit rate**: Expected 40-60% for common assignments (students write similar essays)

#### Batch Processing

**Process non-urgent grading in background:**
```csharp
public async Task QueueEssayForGradingAsync(Guid responseId)
{
    // Add to background job queue
    await _backgroundJobQueue.EnqueueAsync(new GradeEssayJob {
        ResponseId = responseId,
        Priority = Priority.Normal,
        ScheduledAt = DateTime.UtcNow.AddMinutes(5)  // Delay to allow batching
    });
}

// Background worker processes batches
public async Task ProcessBatchAsync(IEnumerable<GradeEssayJob> jobs)
{
    // Process 10 essays in parallel (with rate limiting)
    var tasks = jobs.Take(10).Select(job => GradeEssayAsync(job));
    await Task.WhenAll(tasks);
}
```

#### Fallback to Stub Service

**Use StubLLMService for development/testing:**
```csharp
// appsettings.Development.json
{
  "LLM": {
    "Provider": "Stub",  // Use stub for fast development
    "FallbackOnError": true  // Fallback to stub if OLLAMA unavailable
  }
}

// Production: Use OLLAMA or Azure OpenAI
{
  "LLM": {
    "Provider": "OLLAMA",  // or "AzureOpenAI"
    "FallbackOnError": true  // Fallback to stub on critical failures
  }
}
```

### 4. Async and Parallel Processing

#### Parallel Task Execution

**Execute independent tasks in parallel:**
```csharp
public async Task<StudentDashboard> LoadDashboardAsync(Guid studentId)
{
    // ‚ùå BAD: Sequential execution (1500ms total)
    var summary = await GetPerformanceSummaryAsync(studentId);      // 500ms
    var subjects = await GetSubjectPerformanceAsync(studentId);     // 400ms
    var weakAreas = await GetImprovementAreasAsync(studentId);      // 600ms
    
    // ‚úÖ GOOD: Parallel execution (600ms total - longest task)
    var (summary, subjects, weakAreas) = await (
        GetPerformanceSummaryAsync(studentId),
        GetSubjectPerformanceAsync(studentId),
        GetImprovementAreasAsync(studentId)
    );
    
    return new StudentDashboard { Summary = summary, Subjects = subjects, WeakAreas = weakAreas };
}
```

#### Streaming Responses

**Stream large datasets to reduce memory usage:**
```csharp
public async IAsyncEnumerable<AssessmentResponse> GetResponsesAsync(Guid sessionId, [EnumeratorCancellation] CancellationToken ct = default)
{
    await foreach (var response in _context.AssessmentResponses
        .Where(r => r.SessionId == sessionId)
        .AsAsyncEnumerable()
        .WithCancellation(ct))
    {
        yield return response;
    }
}
```

### 5. Response Compression

**Enable response compression for large payloads:**
```csharp
builder.Services.AddResponseCompression(options =>
{
    options.EnableForHttps = true;
    options.Providers.Add<GzipCompressionProvider>();
    options.Providers.Add<BrotliCompressionProvider>();
    options.MimeTypes = ResponseCompressionDefaults.MimeTypes.Concat(new[] { "application/json" });
});

builder.Services.Configure<GzipCompressionProviderOptions>(options =>
{
    options.Level = CompressionLevel.Fastest;  // Balance speed vs. size
});
```

**Expected savings**: 60-80% bandwidth reduction for JSON responses

### 6. Pagination and Filtering

**Always paginate large result sets:**
```csharp
public async Task<PagedResult<Assessment>> GetAssessmentsAsync(int page = 1, int pageSize = 20, string? subject = null)
{
    var query = _context.Assessments.AsQueryable();
    
    // Apply filters
    if (subject != null)
        query = query.Where(a => a.Subject == subject);
    
    // Get total count (for pagination metadata)
    var totalCount = await query.CountAsync();
    
    // Apply pagination
    var items = await query
        .OrderByDescending(a => a.CreatedAt)
        .Skip((page - 1) * pageSize)
        .Take(pageSize)
        .ToListAsync();
    
    return new PagedResult<Assessment> {
        Items = items,
        TotalCount = totalCount,
        Page = page,
        PageSize = pageSize
    };
}
```

---

## Load Testing Results

### Current Status

**Load Testing Status**: ‚è≥ **0% Complete** (Not yet performed)

**Priority**: üéØ **High** (Required before production deployment)

### Planned Load Tests

#### Test 1: API Endpoint Performance

**Tool**: Apache Bench (ab) or k6

**Scenario**: Measure API response times under load

**Test Cases:**
```bash
# Test 1: Assessment list endpoint (100 concurrent users, 1000 requests)
k6 run --vus 100 --iterations 1000 test-assessment-list.js

# Test 2: Student analytics (50 concurrent users, 500 requests)
k6 run --vus 50 --iterations 500 test-analytics.js

# Test 3: Auto-save (200 concurrent users, 2000 requests)
k6 run --vus 200 --iterations 2000 test-auto-save.js
```

**Expected Results:**
- P50 latency: < 300ms
- P95 latency: < 700ms
- P99 latency: < 1.5s
- Error rate: < 0.5%
- Throughput: > 500 requests/sec

#### Test 2: Concurrent Assessment Taking

**Scenario**: Simulate 1000 students taking assessments simultaneously

**Test Parameters:**
- Ramp-up: 0 ‚Üí 1000 users over 5 minutes
- Duration: 30 minutes sustained load
- Think time: 10-30 seconds between actions (realistic student behavior)

**Expected Results:**
- All 1000 students complete assessments successfully
- No session timeouts
- Auto-save works reliably
- Submit assessment succeeds for 100% of users
- Database connections remain stable (< 80% pool utilization)

#### Test 3: LLM Grading Under Load

**Scenario**: Grade 100 essays concurrently with OLLAMA

**Test Parameters:**
- 100 essay submissions queued
- OLLAMA processing capacity: 20 concurrent requests (configured limit)
- Expected processing time: 20-25 seconds per essay

**Expected Results:**
- All 100 essays graded successfully
- Average time to grade: < 30 seconds
- Queue depth remains manageable (< 50)
- No timeouts or failures
- CPU utilization stays below 80% (to avoid throttling)

#### Test 4: SignalR Real-Time Updates

**Scenario**: Monitor 500 concurrent students taking assessments with real-time progress updates

**Test Parameters:**
- 500 SignalR connections established
- Progress updates sent every 5 seconds
- Test duration: 10 minutes

**Expected Results:**
- All connections remain stable
- Message delivery latency < 200ms
- No dropped connections
- CPU and memory usage remain stable
- Redis backplane handles load without issues

### Load Testing Tools

**Recommended Tools:**

1. **k6** (https://k6.io/):
   - JavaScript-based load testing
   - Excellent for HTTP/REST APIs
   - Great reporting and visualization
   
2. **NBomber** (.NET):
   - Native .NET integration
   - Supports HTTP, WebSockets, gRPC
   - Good for SignalR testing
   
3. **Apache Bench** (ab):
   - Simple HTTP benchmarking
   - Built-in to most Linux distributions
   - Good for quick tests
   
4. **Grafana K6 Cloud**:
   - Cloud-based load testing
   - Distributed load generation
   - Advanced analytics and reporting

---

## Performance Monitoring

### 1. Application Performance Monitoring (APM)

**OpenTelemetry Integration:**
```csharp
builder.Services.AddOpenTelemetry()
    .WithMetrics(metrics =>
    {
        metrics.AddAspNetCoreInstrumentation();
        metrics.AddHttpClientInstrumentation();
        metrics.AddRuntimeInstrumentation();
        metrics.AddPrometheusExporter();
    })
    .WithTracing(tracing =>
    {
        tracing.AddAspNetCoreInstrumentation();
        tracing.AddHttpClientInstrumentation();
        tracing.AddNpgsql();
        tracing.AddRedisInstrumentation();
    });
```

**Key Metrics to Track:**

| Metric | Description | Alert Threshold |
|--------|-------------|-----------------|
| `http_server_request_duration_seconds` | API request duration histogram | P95 > 1s |
| `db_query_duration_seconds` | Database query duration | P95 > 500ms |
| `cache_operations_total` | Redis operations counter | Miss rate > 40% |
| `llm_call_duration_seconds` | LLM evaluation duration | P95 > 60s |
| `agent_task_assignment_duration_seconds` | Agent task routing time | P95 > 3s |
| `http_server_errors_total` | HTTP error count | Rate > 1% |

### 2. Distributed Tracing

**Trace Request Flow Across Services:**
- Client request ‚Üí Web API ‚Üí Database query ‚Üí Cache lookup ‚Üí LLM call
- Identify bottlenecks (slow spans)
- Correlate logs and metrics using trace IDs

**Example Trace:**
```
Request: POST /api/v1/assessments/123/submit
  ‚îú‚îÄ Span: ValidateSubmission (50ms)
  ‚îú‚îÄ Span: SaveResponsesToDatabase (200ms) ‚ö†Ô∏è SLOW
  ‚îÇ   ‚îú‚îÄ Query: INSERT assessment_responses (180ms)
  ‚îÇ   ‚îî‚îÄ Query: UPDATE assessment_session (20ms)
  ‚îú‚îÄ Span: CalculateScore (100ms)
  ‚îî‚îÄ Span: SendNotification (30ms)
Total: 380ms
```

### 3. Real-Time Dashboards

**Grafana Dashboard Panels:**

1. **System Health Overview**:
   - HTTP request rate (requests/sec)
   - Error rate (%)
   - Average response time
   - Active connections

2. **Performance Trends**:
   - P50, P95, P99 latency over time
   - Throughput trends
   - Error rate trends

3. **Resource Utilization**:
   - CPU usage per container
   - Memory usage per container
   - Database connection pool utilization
   - Redis memory usage

4. **LLM Performance**:
   - LLM call duration (p50, p95, p99)
   - LLM calls per minute
   - Queue depth
   - Success rate

### 4. Alerting

**Alert Rules (Azure Monitor or Prometheus):**

```yaml
groups:
  - name: performance_alerts
    rules:
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, http_server_request_duration_seconds) > 1
        for: 5m
        annotations:
          summary: "High API latency detected"
          description: "P95 latency is {{ $value }}s (threshold: 1s)"
      
      - alert: HighErrorRate
        expr: rate(http_server_errors_total[5m]) > 0.01
        for: 5m
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% (threshold: 1%)"
      
      - alert: DatabaseSlowQueries
        expr: histogram_quantile(0.95, db_query_duration_seconds) > 0.5
        for: 5m
        annotations:
          summary: "Slow database queries detected"
          description: "P95 query time is {{ $value }}s (threshold: 500ms)"
```

---

## Bottleneck Analysis

### Identified Bottlenecks

#### 1. LLM Evaluation Latency

**Problem**: OLLAMA on CPU takes 20-25 seconds per essay evaluation

**Impact**: Students wait 20+ seconds for essay grading

**Solutions**:
1. ‚úÖ **Immediate**: Use StubLLMService for development (instant response)
2. üéØ **Short-term**: Add GPU to OLLAMA deployment (2-3s per evaluation)
3. üéØ **Long-term**: Migrate to Azure OpenAI GPT-4o (1.5-2s per evaluation)
4. ‚úÖ **Optimization**: Cache common evaluations (40-60% hit rate expected)
5. ‚úÖ **Optimization**: Batch non-urgent grading as background jobs

**Status**: ‚ö†Ô∏è Acceptable for development, üö® blocker for production at scale

#### 2. Database Query Performance (Projected)

**Problem**: Stub data performs < 100ms, but real database queries will be slower

**Impact**: API response times may exceed targets with real data

**Solutions**:
1. üéØ **Add indexes** on frequently queried columns
2. üéØ **Optimize N+1 queries** with batch loading
3. üéØ **Use compiled queries** for hot paths
4. üéØ **Enable query caching** (Redis) for expensive aggregations
5. üéØ **Consider read replicas** for analytics queries

**Status**: ‚è≥ Not yet tested with real data (high priority)

#### 3. Redis Memory Evictions

**Problem**: 125 evictions per hour with current TTL settings

**Impact**: Cache misses ‚Üí more database queries ‚Üí higher latency

**Solutions**:
1. ‚úÖ **Increase Redis memory** (scale up tier)
2. üéØ **Tune TTLs** (reduce TTL for less-accessed data)
3. üéØ **Implement LRU eviction policy** (evict least recently used)
4. üéØ **Review cache key patterns** (identify large keys)

**Status**: ‚ö†Ô∏è Needs tuning before production

#### 4. SignalR Connection Scaling (Projected)

**Problem**: Single server can handle ~10,000 connections, but enterprise scale needs 50,000+

**Impact**: Connection limit reached, new users cannot connect

**Solutions**:
1. üéØ **Redis backplane** for horizontal scaling (share connections across replicas)
2. üéØ **Azure SignalR Service** (fully managed, 100,000+ connections)
3. üéØ **Connection-based rate limiting** to prevent abuse

**Status**: ‚è≥ Not an issue yet, but plan for enterprise scale

---

## Future Improvements

### Short-Term (Q1 2026)

1. **Complete Load Testing**:
   - Perform all 4 load test scenarios
   - Identify actual bottlenecks with real data
   - Tune configuration based on results

2. **Database Optimization**:
   - Add all recommended indexes
   - Optimize identified N+1 queries
   - Implement compiled queries for hot paths

3. **LLM Migration**:
   - Add GPU to OLLAMA deployment OR
   - Migrate to Azure OpenAI GPT-4o
   - Target: < 3s per evaluation

4. **Cache Tuning**:
   - Adjust TTLs based on usage patterns
   - Increase Redis memory to 6 GB tier
   - Target: < 50 evictions per hour, > 85% hit rate

### Medium-Term (Q2-Q3 2026)

1. **Materialized Views for Analytics**:
   - Pre-aggregate student performance data
   - Refresh hourly (background job)
   - Target: Analytics queries < 200ms (from 500ms+)

2. **CDN Integration**:
   - Serve static assets from Azure CDN
   - Reduce origin load by 80-90%
   - Improve global latency (< 100ms worldwide)

3. **Database Read Replicas**:
   - Route analytics queries to read replicas
   - Reduce load on primary database
   - Enable regional deployments

4. **Advanced Caching**:
   - Cache warming on application startup
   - Predictive cache pre-population (ML-based)
   - Target: > 90% cache hit rate

### Long-Term (2027+)

1. **Multi-Region Deployment**:
   - Deploy to US, Europe, Asia-Pacific
   - Global load balancing with Azure Front Door
   - Target: < 100ms latency for all users

2. **Kubernetes Migration (if needed)**:
   - Migrate from Container Apps to AKS
   - Implement service mesh (Istio or Linkerd)
   - Advanced traffic management and canary deployments

3. **Machine Learning Optimization**:
   - Predictive auto-scaling based on usage patterns
   - Anomaly detection for performance degradation
   - Intelligent query optimization recommendations

4. **Edge Computing**:
   - Deploy edge nodes for LLM inference
   - Reduce latency for essay grading to < 1s
   - Improve user experience globally

---

## Related Documentation

- **System Architecture**: `.github/specification/02-system-architecture.md`
- **Observability**: `.github/specification/08-observability.md`
- **Deployment Models**: `.github/specification/13-deployment-models.md` (next document)
- **Azure Deployment Strategy**: `docs/deployment/AZURE_DEPLOYMENT_STRATEGY.md`
- **Local Deployment Guide**: `docs/deployment/LOCAL_DEPLOYMENT_GUIDE.md`
- **Testing Strategy**: `docs/development/testing/TESTING_STRATEGY.md`

---

**Document Status:** Complete  
**Last Updated:** 2025-01-20  
**Version:** 1.0  
**Contributors:** GitHub Copilot
