name: OLLAMA Integration Tests

# Run nightly and manually only (not on PRs to speed up feedback)
# These tests validate real AI behavior with actual LLM responses
# Main CI pipeline uses StubLLMService for fast, reliable testing
on:
  workflow_dispatch: # Manual trigger
    inputs:
      model:
        description: "OLLAMA model to use"
        required: false
        default: "llama3.2:3b"
  schedule:
    - cron: "0 2 * * *" # Run at 2 AM UTC daily

env:
  DOTNET_VERSION: "9.0.x"
  OLLAMA_MODEL: ${{ github.event.inputs.model || 'llama3.2:3b' }}

jobs:
  ollama-integration-test:
    runs-on: ubuntu-latest
    timeout-minutes: 45 # Increased timeout for Ollama model download and inference

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: Install .NET Aspire workload
        run: dotnet workload install aspire

      - name: Install OLLAMA
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          echo "OLLAMA installed successfully"

      - name: Start OLLAMA service
        run: |
          echo "Starting Ollama service in background..."
          ollama serve > /tmp/ollama.log 2>&1 &
          sleep 5
          echo "OLLAMA service started"

      - name: Pull OLLAMA model (before starting services)
        run: |
          echo "Pulling model: ${{ env.OLLAMA_MODEL }}"
          # Pull the model before Aspire starts (avoids timeout during AppHost startup)
          ollama pull ${{ env.OLLAMA_MODEL }}
          echo "Model pulled successfully"
        timeout-minutes: 15

      - name: Verify OLLAMA is running
        run: |
          ollama --version
          ollama list
          # Verify Ollama API is responding
          curl -sf http://localhost:11434/api/tags || (echo "Ollama not responding" && cat /tmp/ollama.log && exit 1)
          echo "‚úÖ OLLAMA model ${{ env.OLLAMA_MODEL }} is ready and responding"

      - name: Setup test infrastructure with Aspire
        run: |
          echo "Aspire will orchestrate PostgreSQL and Redis automatically"
          echo "No manual docker-compose or service containers needed"

      - name: Restore dependencies
        run: dotnet restore EduMind.AI.sln

      - name: Build solution
        run: dotnet build EduMind.AI.sln --configuration Release --no-restore

      - name: Start Aspire AppHost in background
        env:
          LLM__Provider: Ollama
          Ollama__BaseUrl: "http://localhost:11434"
          Ollama__ModelName: ${{ env.OLLAMA_MODEL }}
          ASPNETCORE_ENVIRONMENT: Testing
          DOTNET_ENVIRONMENT: Testing
        run: |
          echo "Starting Aspire AppHost with ASPNETCORE_ENVIRONMENT=Testing..."
          echo "This should skip Ollama container and use system-installed Ollama"
          cd src/EduMind.AppHost
          # Pass environment explicitly through command line as well
          ASPNETCORE_ENVIRONMENT=Testing DOTNET_ENVIRONMENT=Testing dotnet run --no-build --configuration Release &
          APPHOST_PID=$!
          echo "APPHOST_PID=$APPHOST_PID" >> $GITHUB_ENV

          # Wait for AppHost to start services (Ollama will be started by Aspire)
          echo "Waiting for services to be ready (including Ollama startup)..."
          MAX_WAIT=120  # 4 minutes for all services including Ollama
          for i in $(seq 1 $MAX_WAIT); do
            # Check if Web API health endpoint is responding
            if curl -sf http://localhost:5103/health > /dev/null 2>&1; then
              echo "‚úÖ Services ready!"
              # Verify Ollama is accessible
              if curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; then
                echo "‚úÖ Ollama is responding!"
                break
              else
                echo "‚ö†Ô∏è Web API ready but Ollama not responding yet..."
              fi
            fi
            if [ $((i % 10)) -eq 0 ]; then
              echo "Waiting for services... ($i/$MAX_WAIT seconds)"
            fi
            sleep 1
          done

          # Final health check
          if ! curl -sf http://localhost:5103/health > /dev/null 2>&1; then
            echo "‚ùå Services failed to start within timeout"
            # Show recent logs for debugging
            docker logs $(docker ps -q) 2>&1 | tail -50 || true
            exit 1
          fi

          echo "‚úÖ All services are running and healthy"

      - name: Run integration tests with OLLAMA
        env:
          LLM__Provider: Ollama # Use real OLLAMA service
          Ollama__BaseUrl: "http://localhost:11434"
          Ollama__ModelName: ${{ env.OLLAMA_MODEL }}
          ASPNETCORE_ENVIRONMENT: Testing
        run: |
          dotnet test tests/AcademicAssessment.Tests.Integration \
            --configuration Release \
            --no-build \
            --verbosity normal \
            --logger "trx;LogFileName=ollama-integration-results.trx"

      - name: Run OLLAMA test script
        env:
          API_BASE: "http://localhost:5103/api/v1"
          OLLAMA_BASE: "http://localhost:11434"
        run: |
          chmod +x tests/test-multi-agent-ollama.sh
          # Install bc for floating point comparisons
          sudo apt-get update && sudo apt-get install -y bc
          # Run the test script
          tests/test-multi-agent-ollama.sh || true  # Don't fail on script errors
        timeout-minutes: 10

      - name: Upload OLLAMA test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ollama-test-results
          path: |
            **/*ollama-integration-results.trx
            /tmp/ollama-test-full.log
          retention-days: 30

      - name: Publish test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: OLLAMA Integration Tests
          path: "**/ollama-integration-results.trx"
          reporter: dotnet-trx
          fail-on-error: false # Don't fail on slow OLLAMA tests

      - name: Cleanup
        if: always()
        run: |
          # Stop Aspire AppHost (kills all orchestrated services)
          kill $APPHOST_PID || true
          pkill -f "EduMind.AppHost" || true
          pkill -f ollama || true
          # Aspire cleanup
          docker ps -q | xargs -r docker stop || true

      - name: Comment on PR with OLLAMA results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            let comment = '## ü§ñ OLLAMA Integration Test Results\n\n';

            try {
              const logContent = fs.readFileSync('/tmp/ollama-test-full.log', 'utf8');
              const lines = logContent.split('\n');
              
              // Extract summary
              const summary = lines.filter(line => line.includes('‚úÖ') || line.includes('‚ùå'));
              comment += summary.join('\n');
              
              comment += '\n\n**Note:** OLLAMA tests use CPU inference and may be slow (20-60s per evaluation).';
            } catch (error) {
              comment += '‚ö†Ô∏è Could not read OLLAMA test results.\n';
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
